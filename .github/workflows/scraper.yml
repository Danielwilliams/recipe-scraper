name: Recipe Scraper

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 0'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download en_core_web_sm
      
      - name: Verify installed versions
        run: |
          echo "Installed package versions:"
          pip list | grep -E "requests|beautifulsoup4|cloudscraper|lxml|spacy|psycopg2-binary|python-dotenv"
      
      - name: Check Python syntax
        run: |
          echo "Checking syntax of Python files..."
          python -m py_compile *.py scrapers/*.py database/*.py processors/*.py || exit 1
      
      - name: Test secret access
        env:
          TEST_DB_NAME: ${{ secrets.DB_NAME }}
          TEST_DB_USER: ${{ secrets.DB_USER }}
          TEST_DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          TEST_DB_HOST: ${{ secrets.DB_HOST }}
          TEST_DB_PORT: ${{ secrets.DB_PORT }}
          TEST_DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          echo "Testing database secrets..."
          if [ -n "$TEST_DATABASE_URL" ] || ([ -n "$TEST_DB_NAME" ] && [ -n "$TEST_DB_USER" ] && [ -n "$TEST_DB_PASSWORD" ] && [ -n "$TEST_DB_HOST" ] && [ -n "$TEST_DB_PORT" ]); then
            echo "✅ Database secrets are accessible"
          else
            echo "❌ Some database secrets are NOT accessible"
            exit 1
          fi
      
      - name: Run recipe scraper
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
        run: |
          echo "Running recipe scraper with database config"
          python main.py --source websites --limit 50 || echo "Scraper failed, continuing to save logs"
      
      - name: Run SimplyRecipes scraper specifically
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
        run: |
          echo "Running SimplyRecipes scraper specifically"
          python main.py --source simplyrecipes --limit 20 || echo "SimplyRecipes scraper failed, continuing to save logs"
      
      - name: Upload logs and debug files
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            recipe_scraper.log
            simplyrecipes_scraper.log
            foodnetwork_error_response_*.html
            foodnetwork_debug_page_*.html
            data/recipe_cache/*.html
          if-no-files-found: warn
      
      - name: Commit and push changes
        if: always()
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"
          git add data/* recipe_scraper.log simplyrecipes_scraper.log || echo "No files to add"
          git commit -m "Update scraped recipe data and logs" || echo "No changes to commit"
          git push || echo "Push failed, likely no changes"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}